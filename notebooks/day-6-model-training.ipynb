{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ab8a75",
   "metadata": {},
   "source": [
    "### Model Training (3 Parts/Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90875b1f",
   "metadata": {},
   "source": [
    "Import packages and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9b7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for training\n",
    "import pandas as pd\n",
    "import numpy as np # Good to have for ML tasks\n",
    "import os # Import the os module\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score # Use later for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a759e404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train loaded successfully. Shape: (5634, 32)\n",
      "X_test loaded successfully. Shape: (1409, 32)\n",
      "\n",
      "y_train loaded successfully. Shape: (5634,)\n",
      "y_test loaded successfully. Shape: (1409,)\n",
      "\n",
      "All datasets loaded and y_train/y_test converted to Series.\n"
     ]
    }
   ],
   "source": [
    "# Define the base path components\n",
    "base_project_folder = r\"C:\\Users\\comat\\GitProjects\\customer-churn-ai\" # Raw string is fine here\n",
    "data_subfolder = \"data\"\n",
    "training_input_subfolder_name = \"training_input\"\n",
    "\n",
    "# Construct the path to the 'training_input' directory robustly\n",
    "training_input_path = os.path.join(base_project_folder, data_subfolder, training_input_subfolder_name)\n",
    "\n",
    "# Define full file paths using os.path.join(). (these should match what you saved them as)\n",
    "x_train_path = os.path.join(training_input_path, \"X_train.parquet\")\n",
    "x_test_path = os.path.join(training_input_path, \"X_test.parquet\")\n",
    "y_train_path = os.path.join(training_input_path, \"y_train.parquet\")\n",
    "y_test_path = os.path.join(training_input_path, \"y_test.parquet\")\n",
    "\n",
    "try:\n",
    "    # Load DataFrames (X_train, X_test)\n",
    "    X_train = pd.read_parquet(x_train_path)\n",
    "    X_test = pd.read_parquet(x_test_path)\n",
    "    print(f\"X_train loaded successfully. Shape: {X_train.shape}\")\n",
    "    print(f\"X_test loaded successfully. Shape: {X_test.shape}\")\n",
    "\n",
    "    # Load y_train and y_test (they were saved as DataFrames with a 'Churn' column)\n",
    "    y_train_df = pd.read_parquet(y_train_path)\n",
    "    y_test_df = pd.read_parquet(y_test_path)\n",
    "\n",
    "    # Convert y_train and y_test back to Pandas Series for scikit-learn\n",
    "    if 'Churn' in y_train_df.columns and 'Churn' in y_test_df.columns:\n",
    "        y_train = y_train_df['Churn']\n",
    "        y_test = y_test_df['Churn']\n",
    "        print(f\"\\ny_train loaded successfully. Shape: {y_train.shape}\")\n",
    "        print(f\"y_test loaded successfully. Shape: {y_test.shape}\")\n",
    "        print(\"\\nAll datasets loaded and y_train/y_test converted to Series.\")\n",
    "    else:\n",
    "        print(\"Error: 'Churn' column not found in loaded y_train_df or y_test_df.\")\n",
    "        # Handle error or stop if target is not loaded correctly\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: One or more Parquet files not found. Please check paths:\")\n",
    "    print(f\"  X_train expected at: {x_train_path}\")\n",
    "    print(f\"  X_test expected at: {x_test_path}\")\n",
    "    print(f\"  y_train expected at: {y_train_path}\")\n",
    "    print(f\"  y_test expected at: {y_test_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d6ae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Head of X_train:\n",
      "   SeniorCitizen  MonthlyCharges  TotalCharges    HF_neg    HF_nue    HF_pos  \\\n",
      "0              0       -0.521976     -0.263871 -0.596856 -0.608280  0.631591   \n",
      "1              0        0.337478     -0.505423 -0.594176  0.155381  0.535430   \n",
      "2              0       -0.809013     -0.751850 -0.578528  2.236056  0.265643   \n",
      "3              0        0.284384     -0.174271 -0.599526 -0.571609  0.629585   \n",
      "4              0       -0.676279     -0.991514 -0.595537 -0.536079  0.621505   \n",
      "\n",
      "   gender_Male  Partner_Yes  Dependents_Yes  PhoneService_Yes  ...  \\\n",
      "0         True        False           False             False  ...   \n",
      "1         True         True            True              True  ...   \n",
      "2         True         True            True             False  ...   \n",
      "3        False         True           False              True  ...   \n",
      "4         True         True            True              True  ...   \n",
      "\n",
      "   PaymentMethod_Credit card (automatic)  PaymentMethod_Electronic check  \\\n",
      "0                                  False                            True   \n",
      "1                                  False                           False   \n",
      "2                                  False                           False   \n",
      "3                                   True                           False   \n",
      "4                                  False                            True   \n",
      "\n",
      "   PaymentMethod_Mailed check  HF_Label_Neutral  HF_Label_Positive  \\\n",
      "0                       False             False               True   \n",
      "1                        True             False               True   \n",
      "2                        True             False               True   \n",
      "3                       False             False               True   \n",
      "4                       False             False               True   \n",
      "\n",
      "   TenureGrp_13-24 Months  TenureGrp_25-36 Months  TenureGrp_37-48 Months  \\\n",
      "0                   False                    True                   False   \n",
      "1                    True                   False                   False   \n",
      "2                    True                   False                   False   \n",
      "3                   False                    True                   False   \n",
      "4                   False                   False                   False   \n",
      "\n",
      "   TenureGrp_49-60 Months  TenureGrp_61-72 Months  \n",
      "0                   False                   False  \n",
      "1                   False                   False  \n",
      "2                   False                   False  \n",
      "3                   False                   False  \n",
      "4                   False                   False  \n",
      "\n",
      "[5 rows x 32 columns]\n",
      "\n",
      "Head of y_train:\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: Churn, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the head of X_train and first 5 of y_train to verify\n",
    "print(\"\\nHead of X_train:\")\n",
    "print(X_train.head())\n",
    "print(\"\\nHead of y_train:\")\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0599d4fb",
   "metadata": {},
   "source": [
    "---\n",
    "#### Model 1: Train Logistic Regression Model\n",
    "Initialize Logistic Regression Model and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2648780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model initialized with solver='liblinear' and max_iter=1000.\n",
      "\n",
      "Training the Logistic Regression model...\n",
      "Logistic Regression model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the Logistic Regression Model\n",
    "# Begin with mostly default parameters. `liblinear` solver used for binary classification and small datasets.\n",
    "# `random_state` used for reproducibility of results if solver involves randomness.\n",
    "# `max_iter` set to 1000 to ensure convergence for complex datasets. default is 100.\n",
    "log_reg_model = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)\n",
    "print(\"Logistic Regression model initialized with solver='liblinear' and max_iter=1000.\")\n",
    "\n",
    "# 2. Fit/Train the model using the training data.\n",
    "print(\"\\nTraining the Logistic Regression model...\")\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a3e47e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check: Training Accuracy for Logistic Regression: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# QUICK Check on training accuracy [NOT a substitute for test set evaluation!]\n",
    "# This just tells us how well the model fits the data it learned from.\n",
    "y_train_pred_log_reg = log_reg_model.predict(X_train)\n",
    "train_accuracy_log_reg = accuracy_score(y_train, y_train_pred_log_reg)\n",
    "print(f\"Quick check: Training Accuracy for Logistic Regression: {train_accuracy_log_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383a6bd",
   "metadata": {},
   "source": [
    "---\n",
    "#### Insights / Notes: Logistic Regression\n",
    "* Training accuracy is 1.0, which means the model learned the data so well that it makes no mistakes. This is not necessarily good.  \n",
    "\n",
    "* **Potential Implications**:\n",
    "    * **Overfitting**:\n",
    "        - *Definition*: when a machine learning model learns the training data too specifically, capturing not only the underlying patterns but also the noise and random fluctuations unique to that particular set of data.\n",
    "        - *Result*: model becomes very good at predicting the data it has already seen but performs poorly when it encouter new, unseen data (which will be X_test). This is an issue because an overfit model has poor generization, which is the goal of a predictive model, and will lead to misleading sense of real-world performance.\n",
    "        * *Analogy*:\n",
    "            * Overfitting is like a student who has memorized the exact answers to every single question in the practice textbook (X_train, y_train), so they can ace any test that uses those exact questions.\n",
    "            * However, if the actual exam (X_test, y_test) has slightly different questions or new scenarios (even if based on the same overall subject), the student who only memorized will struggle because they didn't learn the underlying concepts needed to solve new problems.\n",
    "            * We want a student (model) who learns the concepts well enough to perform well on both practice questions and the real exam.  \n",
    "\n",
    "* **Plan**:\n",
    "    * Test log_reg_model on X_test and y_test data, which it hasn't seen before.\n",
    "    * Train Other Models like Random Forest and an XGBoost model. They may behave differently on the training so I can compare their results. Choose one that generalizes best  \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e86ff8",
   "metadata": {},
   "source": [
    "#### Model 2: Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf6b506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier model initialized with n_estimators=100 and n_jobs=-1\n",
      "\n",
      "Training the Random Forest Regression model...(get comfy)\n",
      "Random Forest model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. Initialize the Random Forest Classifier model\n",
    "# Start with common default values; set a random_state for reproducibility.\n",
    "# n_estimators is the number of trees in the forest. n_jobs=-1 uses all available CPU cores for training to boost speed.\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "print(\"Random Forest Classifier model initialized with n_estimators=100 and n_jobs=-1\")\n",
    "\n",
    "# 2. Fit/Train the model using the training data.\n",
    "print(\"\\nTraining the Random Forest Regression model...(get comfy)\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Random Forest model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87db369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quick check: Training Accuracy for Random Forest: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# QUICK Check on training accuracy [NOT a substitute for test set evaluation!]\n",
    "# This just tells us how well the model fits the data it learned from.\n",
    "try:\n",
    "    y_train_pred_rf = rf_model.predict(X_train)\n",
    "    train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "    print(f\"\\nQuick check: Training Accuracy for Random Forest: {train_accuracy_rf:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate training score: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cea447",
   "metadata": {},
   "source": [
    "---\n",
    "#### Insights / Notes: Random Forest\n",
    "- Training accuracy is 1.0, which means the model learned the X_train data so well that it makes no mistakes when predicting y_train.\n",
    "- While this shows the model has high capacity to learn, it doesn't tell us how well it will perform on new, unseen data. The primary concern remains overfitting.\n",
    "\n",
    "* **Potential Implications**:\n",
    "    * *Model Complexity*: Random Forests are powerful ensemble models. By default, each of the 100 decision trees in your forest can grow quite deep and complex. Individual trees can become very good at fitting (even \"memorizing\") the specific portion of the data they are trained on.\n",
    "    * *Ensemble Effect*: When you have 100 trees voting, if many of them have perfectly learned their part of the training data, the overall ensemble can easily achieve perfect accuracy on that same training data.\n",
    "    * Potential Overfittiing  \n",
    "  \n",
    "* **Analogy** (Revisiting our Student Analogy):\n",
    "    * If Logistic Regression was like one student who memorized the practice test answers, your Random Forest is like an entire study group where each member memorized a different section of the practice textbook or different versions of practice questions.\n",
    "        * When they collaborate (the \"ensemble\" of trees), they can collectively answer every single question from those practice materials perfectly.\n",
    "        * However, the real test is the final exam (X_test), which will have questions they haven't seen before. Their ability to generalize their knowledge, rather than just recall memorized answers, will be tested then.\n",
    "\n",
    "* **Plan**:\n",
    "    * Train XGBoost, Evaluate all models, then Test log_reg_model, rf_model, and xgb_model on X_test and y_test data, which they haveb't seen before.  \n",
    "    * Test Set Evaluation is Key: The performance on X_test (on Day 7) will be the true indicator of how well this Random Forest model generalizes. We'll compare its training accuracy to its testing accuracy. A large drop would confirm overfitting.\n",
    "    * Hyperparameter Tuning (Later): If we find that the Random Forest is overfitting (i.e., high training accuracy, much lower test accuracy), we can later tune its hyperparameters. For Random Forest, common parameters to adjust to reduce overfitting include:\n",
    "        * max_depth (limiting how deep each tree can grow)\n",
    "        * min_samples_split (requiring more samples to split a node)\n",
    "        * min_samples_leaf (requiring more samples to be at a leaf node)\n",
    "        * n_estimators (number of trees – sometimes more trees can help, but other times a very large number with overly complex individual trees might still overfit).\n",
    "        * This tuning helps simplify the model and encourage it to learn more general patterns\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893eecc6",
   "metadata": {},
   "source": [
    "#### Model 3: Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b94fc566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost model initialized.\n",
      "\n",
      "Training the XGBoost model... (This might also take a moment)\n",
      "XGBoost model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 1. Initialize the XGBoost Classifier model\n",
    "# We'll set a random_state for reproducibility.\n",
    "# For binary classification, specify objective with 'binary:logistic'.\n",
    "# eval_metric can be set to 'logloss' or 'auc' for classification.\n",
    "xgb_model = XGBClassifier(objective='binary:logistic',\n",
    "                          eval_metric='logloss',    # A common metric for binary classification\n",
    "                          random_state=42)\n",
    "\n",
    "print(\"XGBoost model initialized.\")\n",
    "\n",
    "# 2. Fit/Train the model using the training data.\n",
    "print(\"\\nTraining the XGBoost model... (This might also take a moment)\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"XGBoost model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2538deef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quick check: Training Accuracy for XGBoost: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# QUICK Check on training accuracy [NOT a substitute for test set evaluation!]\n",
    "# This just tells us how well the model fits the data it learned from.\n",
    "try:\n",
    "    y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "    train_accuracy_xgb = accuracy_score(y_train, y_train_pred_xgb)\n",
    "    print(f\"\\nQuick check: Training Accuracy for XGBoost: {train_accuracy_xgb:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate training score: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1da06e",
   "metadata": {},
   "source": [
    "---\n",
    "#### Insights / Notes: XGBoost\n",
    "- Training accuracy is 1.0, which means the model learned the X_train data so well that it makes no mistakes when predicting y_train.\n",
    "- While this shows the model has high capacity to learn, it doesn't tell us how well it will perform on new, unseen data. The primary concern remains overfitting.\n",
    "\n",
    "\n",
    "* **Potential Implications**:\n",
    "    * *Overfitting* is the Recurring Theme.\n",
    "        * The model might have \"memorized\" the training data rather than learning generalizable patterns that would apply to new customers.\n",
    "* **Relevance/Importance**: If the model is overfit, its real-world performance (when predicting churn for new customers or on your test set) will likely be significantly lower than this perfect training score.\n",
    "\n",
    "* **Plan**:\n",
    "    - Move to Day 7 and evaluate models on Test Data to measure generalization performance.\n",
    "    - Evaluate models using F1, AUC, confusion matrix  \n",
    "\n",
    "    * Parameter Tuning (Later): XGBoost has regularization parameters (like lambda for L2, alpha for L1, max_depth, learning_rate, subsample, colsample_bytree) that can be tuned later to combat overfitting if the test set performance indicates it's a problem.\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn-ai-en2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
