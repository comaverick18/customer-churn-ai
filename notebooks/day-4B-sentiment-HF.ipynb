{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b7c6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d7114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e162cbacec3a4bf6a65020dfc54b985c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\comat\\miniconda3\\envs\\churn-ai-en2\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\comat\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-roberta-base-sentiment-latest. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df558a4598e3492b8d555381f4de353e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918fd252b3d9434c96a0d43214f2b35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bb54e67d88429bbf7cb144fcfebdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b49f1f64ad14007978019767ab521b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face sentiment analysis pipeline initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Initialize the sentiment analysis pipeline with our chosen model\n",
    "# This will download the model if you haven't used it before (can take a few minutes and some disk space)\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    ")\n",
    "\n",
    "print(\"Hugging Face sentiment analysis pipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76679c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded Successfully! Sensational!\n"
     ]
    }
   ],
   "source": [
    "clean_file_path = r\"C:\\Users\\comat\\GitProjects\\customer-churn-ai\\data\\cleaned_telco_churn.csv\"\n",
    "\n",
    "try:\n",
    "    df_clean = pd.read_csv(clean_file_path)\n",
    "    print(\"Data Loaded Successfully! Sensational!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {clean_file_path}. Please check the path and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c4ad8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing review texts...\n",
      "Starting sentiment prediction for 7043 reviews... (This may take a few minutes)\n",
      "\n",
      "Processed 7043 reviews with Hugging Face model.\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing review texts...\")\n",
    "# Convert the CustomerReview column to a list, replacing NaN with empty strings\n",
    "# as the pipeline expects string inputs.\n",
    "review_texts = df_clean['CustomerReview'].fillna('').tolist()\n",
    "\n",
    "print(f\"Starting sentiment prediction for {len(review_texts)} reviews... (This may take a few minutes)\")\n",
    "# Get predictions for all texts\n",
    "# This might take some time depending on the number of reviews and your hardware\n",
    "hf_predictions = sentiment_pipeline(review_texts)\n",
    "\n",
    "print(f\"\\nProcessed {len(hf_predictions)} reviews with Hugging Face model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2e3811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Hugging Face predictions (first 3):\n",
      "Review snippet: Really happy with my plan... no unexpected fees. Made the right choice here....\n",
      "Prediction: {'label': 'positive', 'score': 0.9732906222343445}\n",
      "Review snippet: Really happy with my plan... phone support was helpful and polite....\n",
      "Prediction: {'label': 'positive', 'score': 0.9825600981712341}\n",
      "Review snippet: Unfortunately, my service was... frustrated by the slow data....\n",
      "Prediction: {'label': 'negative', 'score': 0.9061986207962036}\n"
     ]
    }
   ],
   "source": [
    "# You can inspect the first few predictions to see their structure if you like:\n",
    "print(\"\\nSample Hugging Face predictions (first 3):\")\n",
    "for i in range(min(3, len(hf_predictions))):\n",
    "    print(f\"Review snippet: {review_texts[i][:100]}...\")\n",
    "    print(f\"Prediction: {hf_predictions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb8e3134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing predictions and adding to DataFrame...\n",
      "Hugging Face sentiment labels and scores added to DataFrame.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProcessing predictions and adding to DataFrame...\")\n",
    "# Extract labels and scores\n",
    "hf_labels_raw = [pred['label'] for pred in hf_predictions]\n",
    "hf_scores = [pred['score'] for pred in hf_predictions]\n",
    "\n",
    "# Define the mapping from the model's output labels to our desired text labels\n",
    "# For cardiffnlp/twitter-roberta-base-sentiment-latest:\n",
    "# 0 -> Negative (LABEL_0)\n",
    "# 1 -> Neutral  (LABEL_1)\n",
    "# 2 -> Positive (LABEL_2)\n",
    "label_mapping = {\n",
    "    \"negative\": \"Negative\",\n",
    "    \"neutral\": \"Neutral\",\n",
    "    \"positive\": \"Positive\"\n",
    "}\n",
    "\n",
    "# Map the raw labels to text labels\n",
    "hf_sentiment_labels = [label_mapping.get(label, \"Unknown\") for label in hf_labels_raw] # Added \"Unknown\" for safety\n",
    "\n",
    "# Add as new columns to your DataFrame\n",
    "df_clean['HF_Sentiment_Label'] = hf_sentiment_labels\n",
    "df_clean['HF_Sentiment_Score'] = hf_scores\n",
    "\n",
    "print(\"Hugging Face sentiment labels and scores added to DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a694cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique raw labels produced by the Hugging Face model: {'neutral', 'negative', 'positive'}\n",
      "First few raw labels: ['positive', 'positive', 'negative', 'positive', 'negative']\n"
     ]
    }
   ],
   "source": [
    "# # --- Insert this cell for debugging ---\n",
    "# # Assuming hf_labels_raw has been created in a previous cell:\n",
    "# # hf_labels_raw = [pred['label'] for pred in hf_predictions]\n",
    "\n",
    "# if 'hf_labels_raw' in locals() and hf_labels_raw: # Check if it exists and is not empty\n",
    "#     unique_raw_labels = set(hf_labels_raw)\n",
    "#     print(f\"Unique raw labels produced by the Hugging Face model: {unique_raw_labels}\")\n",
    "#     print(f\"First few raw labels: {hf_labels_raw[:5]}\")\n",
    "# else:\n",
    "#     print(\"Error: hf_labels_raw is not defined or is empty. Please ensure Step C ran correctly and hf_predictions has data.\")\n",
    "# # --- End of debugging cell ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e87b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of Sentiment Labels:\n",
      "HF_Sentiment_Label\n",
      "Positive    5177\n",
      "Negative    1818\n",
      "Neutral       48\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDistribution of Sentiment Labels:\")\n",
    "print(df_clean[\"HF_Sentiment_Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfcb616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of Sentiment as Percentages:\n",
      "HF_Sentiment_Label\n",
      "Positive    0.735056\n",
      "Negative    0.258129\n",
      "Neutral     0.006815\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDistribution of Sentiment as Percentages:\")\n",
    "print(df_clean[\"HF_Sentiment_Label\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23ef36ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with Hugging Face Sentiment (first 5 rows):\n",
      "                                      CustomerReview HF_Sentiment_Label  \\\n",
      "0  Really happy with my plan... no unexpected fee...           Positive   \n",
      "1  Really happy with my plan... phone support was...           Positive   \n",
      "2  Unfortunately, my service was... frustrated by...           Negative   \n",
      "3  Consistently good signal... data plan is a gre...           Positive   \n",
      "4  Not satisfied with the billing... billing erro...           Negative   \n",
      "\n",
      "   HF_Sentiment_Score  \n",
      "0            0.973291  \n",
      "1            0.982560  \n",
      "2            0.906199  \n",
      "3            0.960772  \n",
      "4            0.930403  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows with the new Hugging Face sentiment columns\n",
    "print(\"\\nDataFrame with Hugging Face Sentiment (first 5 rows):\")\n",
    "print(df_clean[['CustomerReview', 'HF_Sentiment_Label', 'HF_Sentiment_Score']].head()) # Comparing with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the counts of each new sentiment label\n",
    "print(\"\\nDistribution of Hugging Face Sentiment Labels:\")\n",
    "print(df_clean['HF_Sentiment_Label'].value_counts())\n",
    "\n",
    "# Display the percentage distribution (capped at 3 decimals)\n",
    "print(\"\\nDistribution of Hugging Face Sentiment as Percentages:\")\n",
    "hf_percentages = df_clean[\"HF_Sentiment_Label\"].value_counts(normalize=True) * 100\n",
    "print(hf_percentages.round(3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn-ai-en2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
